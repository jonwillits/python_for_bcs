# 11.3. Regression in scikit-learn.

## Installing scikit-learn
So let's get started by installing scikit-learn. Go you your terminal and:
```text
pip3 install scikit-learn
```

## Simple linear regression in scikit-learn
To use scikit-learn to make a linear model of this data is super easy. The only issue is that the data needs to be 
formatted into a matrix with columns for the different variables, and rows for the different observations. So let's 
take our fake random data from the previous example, and then put it into that form. 

First, let's pick our b parameters beforehand, and generate a psuedo-random dataset using them. Remember that normally
we would be using a real dataset from the real world, and we wouldn't know these b-values. We would be trying to figure
them out. But in this case we're doing it this way to see that the regression algorithm works. Let's do a multiple 
regression where we have three predictor variables (x1, x2, and x3) and an intercept x0.

### Generating some pseudo-random data
```python
import numpy as np
from sklearn.linear_model import LinearRegression


# our made-up intercept and three slopes
b0 = -1
b1 = 2
b2 = -3
b3 = 4

n = 20 # the number of observations we will generate

x = np.random.randint(-10, 10, [n, 3])  # generate a 20x3 matrix of random integers between -10 and 10
noise = np.random.randint(-5, 5, [n])  # generate a 1x20 vector of noise to add to the y value so the correlation isnt 1
theta = np.array[b1, b2, b3]  # create an array from our three slopes
y = noise + b0 + np.dot(x, theta)  # compute the 1x20 vector of y from y = b0 + b1*x1 + b2*x2 + b3*x3
```

### Using sklearn to make a linear regression model
```python
lr = LinearRegression(fit_intercept=True)
lr.fit(x, y)
```
And that's it! The LinearRegression class from Sklearn fits a regression model to our data, using the first argument
in the .fit() function to predict the second argument. The first argument can be a matrix of any size, but the 
rows must be the number of observations, and the columns must be the predictor variables. The second argument must be a 
one dimensional array of the same size as the number of rows in the first argument. In other words, if x's size is 
an mXn matrix, then y's size must be an array of size m.

Inside the .fit() function, sklearn is running code that is pretty much exactly like what we used
before. It:
- assign random values to all the b's
- for num_iterations or until error < tolerance:
  - for each set of x's and y:
    - predict y using the current b's
    - compute the error for that prediction
    - adjust the b's to be less wrong in the future

Some options above is `num_iterations` and `tolerance`. That's just how many times you run that loop to adjust the b's.
You can choose to do it a fixed number of times, or until the error score is below the tolerance level. For linear 
regression, even with many predictors, the solution is stable and guaranteed to occur, so you don't need to worry 
about it too much. Whatever sklearn does automatically is fine. But with nonlinear models or more complicated algorithms
we do have to worry aobut these parameters, and if we want to change them you can do so. You can read more about that 
here:
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression

### Inspecting the model
We can examine the output in various ways. We can print the b coefficients found by the model:
```python
print(lr.intercept_, lr.coef_)
```
```text
-1.5538347833740733 [ 1.86896862 -3.01317563  4.11555825]
```
We can see here that we added a fair amount of noise to each y value. Without the noise, y would have been a sum of 
three numbers between -10 and positive 10, multiplied by 2, -3, and 4, with -1 added as the intercept. So y could have
been anywhere from -91 (-1 + -10*2 + 10*-3 + -10*4) to 89 (-1 + 10*2 + -10*-3 + 10*4). We randomly added a number 
between -5 and 5 to each y, which is a fair amount of noise on that scale. So the resulting b parameters found by the 
model were not perfect. But still pretty good and close to the original real values.

We could also use sklearn to get predictions for each individual y value:
```python
y_predict = lr.predict(x)
error = y_predict - y
for i in range(n):
	print("{}    {}    {:0.3f}     {:0.3f}   {:0.3f}".format(x[i], y[i], y_predict[i], error[i], noise[i]))
```
As we can see in the output below, each prediction was pretty good, and the error on each prediction corresponds 
very closely to the noise that we added to each y value when we created them in the first place. So to the extent that
there was structure in the data that could be predicted (the b-values) the linear regression found it pretty well.
```text
x                 y    y_predict    error    noise
[9   4   6]      24     28.423      4.423   -5.000
[-7  -3  -6]    -28    -30.077     -2.077    2.000
[ 3   3  -4]    -20    -19.467      0.533    0.000
[ 0   3 -10]    -48    -48.741     -0.741    2.000
[ 8   7  -6]    -27    -28.656     -1.656    3.000
[ 1  -6   1]     22     22.432      0.432   -1.000
[-6   5  -6]    -51    -51.897     -0.897    1.000
[-1  -7 -10]    -22    -20.944      1.056    0.000
[ 2  -4  -6]    -13     -8.306      4.694   -4.000
[-5   9   9]     -5     -4.230      0.770   -3.000
[-7   8  -9]    -71    -74.456     -3.456    4.000
[-8   2  -7]    -55    -50.942      4.058   -4.000
[-3  -3   0]      1      1.284      0.284   -1.000
[-10 -5   8]     22     23.337      1.337   -4.000
[-1  -8   7]     52     47.249     -4.751    3.000
[-6   0   9]     22     20.578     -1.422   -1.000
[ 4  -2 -10]    -24    -25.451     -1.451    3.000
[ 5  -1   5]     36     31.181     -4.819    4.000
[-4  -1   4]      5      8.563      3.563   -5.000
[ 3  -4  -2]      9      9.123      0.123    0.000
```

We can also get the `R^2` score from the model: hat percentage of the variance in the data could be predicted from the 
parameters:
```python
print(lr.score(x, y))
```
output
```text
0.9953803064300027
```
That's a pretty good R^2 score. You'll never get one that high on real data unless you're studying basic physics, or 
you've screwed up your data. In the real world, data is much noisier than in our simple example.

Next: [11.4. Classification in sklearn](../CH11/11.4.%20Classification%20in%20sklearn.md)<br>
Previous: [11.2 Multiple and Nonlinear Regression](../CH11/11.2.%20Multiple%20and%20Nonlinear%20Regression.md)

