# 12.2. NLP Modules

In addition to the built-in re module, there are a number of other third-party modules for doing natural language 
processing in python:
- `NLTK (Natural Language Toolkit)`: NLTK is one of the most popular NLP libraries for Python. It provides a wide range 
of tools and modules for natural language processing, such as tokenization, stemming, lemmatization, part-of-speech 
tagging, and named entity recognition. NLTK is open-source, easy to use, and has a large community of contributors.
- `spaCy`: spaCy is a popular NLP library that is designed for performance and efficiency. It provides advanced features 
such as entity recognition, dependency parsing, and vectorization. spaCy is known for its speed and accuracy and is 
widely used in industry and academia.
- `TextBlob`: TextBlob is a Python library for processing textual data. It provides a simple interface for performing 
common NLP tasks such as sentiment analysis, part-of-speech tagging, and noun phrase extraction. TextBlob is built on 
top of NLTK and provides an easy-to-use interface for NLP tasks.
- `Gensim`: Gensim is a library for topic modeling and document similarity analysis. It provides tools for unsupervised 
learning of topics from text data, as well as for finding similar documents or passages within a larger corpus.
- `scikit-learn`: scikit-learn, which we have already discussed also provides tools for text analysis. It includes modules 
for text classification, clustering, and feature extraction.

It is beyond the scope of our course to cover all of these tools here. We are going to focus on SpaCy, as it is one of 
the more popular tools in research and industry due to its speed and accuracy.

## Installing spaCy

You can install spaCy like other third party modules:
```text
pip3 install spacy
```

In addition to downloading the spaCy module, you also need to download a language model for it. This is its information 
about the rules of a language it uses to do all of its business. There are spacy language modules for many different 
languages. For many languages they also come in different sizes (small, medium, and large), with bigger sized language 
models being more accurate but also using more disk space and running more slowly. Let's install the medium English 
model. To do this, type the following at the command line:
```text
python -m spacy download en_core_web_md
```

## Tokenization
One of the primary uses of modules like spaCy is tokenization. Tokenization is taking a bunch of text and splitting it 
into a list of words or word-like units (called tokens). We've done this ourselves in earlier chapters while learning 
some python basics, but in the real world we use modules like spaCy to do it for us. Here's an example of loading a file
that contains the first paragraph of the wikipedia entry on Cognitive Science.
```text
Cognitive science is the interdisciplinary, scientific study of the mind and its processes with input from linguistics, 
psychology, neuroscience, philosophy, computer science/artificial intelligence, and anthropology. It examines the 
nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and 
behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern 
to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these 
faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, 
neuroscience, and anthropology. The typical analysis of cognitive science spans many levels of organization, 
from learning and decision to logic and planning; from neural circuitry to modular brain organization. One of the 
fundamental concepts of cognitive science is that "thinking can best be understood in terms of representational 
structures in the mind and computational procedures that operate on those structures."
```

```python
import spacy

nlp = spacy.load("en_core_web_md")
with open("cogsci.txt") as f:
    text = f.read()
tokens = nlp(text)
print(tokens)
```
output
```text
Cognitive science is the interdisciplinary, scientific study of the mind and its processes with input from linguistics, 
psychology, neuroscience, philosophy, computer science/artificial intelligence, and anthropology. It examines the 
nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and 
behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern 
to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these 
faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, 
neuroscience, and anthropology. The typical analysis of cognitive science spans many levels of organization, 
from learning and decision to logic and planning; from neural circuitry to modular brain organization. One of the 
fundamental concepts of cognitive science is that "thinking can best be understood in terms of representational 
structures in the mind and computational procedures that operate on those structures."
```
So it looks as though nothing happened. But actually complex spacy "doc" data structure that just looks like a block of 
text if you print it. We can see that it is actually a sequence if we print the first 20 elements:
```python
for token in tokens[:20]:
    print(token)
```
Output:
```text
Cognitive
science
is
the
interdisciplinary
,
scientific
study
of
the
mind
and
its
processes
with
input
from
linguistics
,
psychology
```

### Properties of spaCy tokens
If this had been the string that it appeared to be, we would have gotten the first 20 characters. Instead, we get the 
first 20 tokens. Notice that it split off the punctuation for us. That's nice! This is why we call it "tokens" and not 
"words". So it looks like a list of tokens. But in fact each token is itself not a string but its own complex data 
structure from which we can extract other information:
- token.text:  the original text of the token
- token.lemma_: the base form of the token (e.g., "run" instead of "running")
- token.pos_: the part-of-speech tag of the token (e.g., "NOUN", "VERB", "ADJ", etc.)
- token.tag_: a more detailed part-of-speech tag (e.g., "NN" for a singular noun, "NNS" for a plural noun, etc.)
- token.dep_: the syntactic dependency label of the token (e.g., "SUBJ", "OBJ", "ADJ", etc.)
- token.is_stop: a boolean flag indicating whether the token is a stop word (i.e., a common word that is often removed 
in text processing for meaningful words)

Let's see:
```python
for token in tokens:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_stop)
```
output:
```text
Cognitive cognitive ADJ JJ amod False
science science NOUN NN nsubj False
is be AUX VBZ ROOT True
the the DET DT det True
interdisciplinary interdisciplinary ADJ JJ amod False
, , PUNCT , punct False
scientific scientific ADJ JJ amod False
study study NOUN NN attr False
of of ADP IN prep True
the the DET DT det True
mind mind NOUN NN pobj False
and and CCONJ CC cc True
its its PRON PRP$ poss True
processes process NOUN NNS conj False
with with ADP IN prep True
input input NOUN NN pobj False
from from ADP IN prep True
linguistics linguistic NOUN NNS pobj False
, , PUNCT , punct False
psychology psychology NOUN NN conj False
```
So here we can see that, for example, .lemma_ removes the plural marker from processes, and tells us that it is a 
noun, a plural noun (that's what NNS stands for as a .tag_), and that it is a dependent of a conjunction (conj for 
token.dep_), (i.e. `the mind and its processes`). All those codes and their definitions can be looked up here:
https://spacy.io/api/data-formats. 

There are all kinds of analyses you might do with words based on using just their stem form (without plurals or 
inflection) or based on knowing their part of speech or dependencies.

### SpaCy is a predictive model; it makes mistakes
One important caveat. SpaCy is itself using machine learning to process a text corpus and turn it into parts of 
speech and dependencies. It is not using simple rules. Consider, for example, the following example:

```python
some_text = "Will you have a drink with me. Will you drink with me?"
tokens = nlp(some_text)
for token in tokens:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_stop)
```
Output:
```text
Will will AUX MD aux True
you you PRON PRP nsubj True
have have VERB VB ROOT True
a a DET DT det True
drink drink NOUN NN dobj False
with with ADP IN prep True
me I PRON PRP pobj True
. . PUNCT . punct False
Will will AUX MD aux True
you you PRON PRP nsubj True
drink drink VERB VB ROOT False
with with ADP IN prep True
me I PRON PRP pobj True
? ? PUNCT . punct False
```
Let's focus on the word "drink" in these two sentences. In the first sentence it is a noun, in the second it is a verb. 
How does spacy know?

There is actually a complex set of classification models like those we talked about last week being used to make 
predictions. The model is trained on some data that is labeled (i.e. telling us what the part of speech of a word is), 
and then uses that to learn a procedure to make guesses about new text. So maybe it learns that tokens that follow 
tokens like "a" and "the" are nouns, whereas tokens that follow pronouns like "you" are verbs. In reality, the model is 
doing something a lot more complex than this, but that's the basic idea.

The fact that spacy is making predictions from a model means that it will sometimes make mistakes. Spacy is known for 
being very accurate, but it is not perfect. You can imagine trying to mess with its predictions by having difficult 
sentences. Take, for example, this classic sentence used in psychology of language experiments:
```python
some_text = "The old man the boats."
tokens = nlp(some_text)
for token in tokens:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_stop)
```
Output:
```text
The the DET DT det True
old old ADJ JJ amod False
man man NOUN NN ROOT False
the the DET DT det True
boats boat NOUN NNS appos False
. . PUNCT . punct False
```
This is called a "syntactic garden path" sentence. This is because your really strong belief after reading the first 
three words is that "man" is a noun, probably because 1) it usually is, in general, and 2) 99.9999% of the time it is 
when it starts a sentence like "the old man". However, in this sentence man is a verb (as in, "to man the boats, which 
means to handle or work on the boat". The word "old" is the noun, as in "the old people handle the boats". But because 
both "old" and "man" are being used in a very unusual way, this sentence is very difficult to parse. Spacy is misled by 
the statistics of language in the same way that people are.

## Part of Speech Tagging
Let's talk a bit more about part of speech (PoS) tagging and why it's useful. Suppose you have a large corpus of text, 
such as a collection of news articles or social media posts, and you want to analyze the frequency and distribution of 
different parts of speech in the text. One way to do this is to use spaCy to perform POS tagging on the text. You might 
also want to identify the nouns in a sentence, so that you can do follow-up analyses just on the nouns.

Here are some use cases of PoS tagging:
- Identify common noun phrases or verb phrases in the text, which can be useful for text summarization or information 
retrieval. 
- Analyze the writing style or tone of the text based on the frequency of certain parts of speech, such as adjectives 
or adverbs. 
- Extract named entities, such as people, organizations, or locations, which can be useful for text classification or 
entity recognition.

Overall, POS tagging is a fundamental task in NLP and is used in many applications, such as text classification, 
machine translation, and information extraction. By understanding the syntactic structure of text, we can gain insights 
into its meaning and use it to train more accurate and effective NLP models.

## Named Entity Recognition
Another common use of spaCy is for a procedure called named entity recognition. Named entity recognition is, like it 
sounds, a function that gives you a list of the names in a spaCy document, as well as the kind of entity that it is.

Suppose you have a large corpus of text, such as news articles or social media posts, and you want to extract named 
entities from the text, such as people, organizations, and locations. One way to do this is to use spaCy's NER 
capabilities.
```python
text = "Barack Obama was born in Hawaii and went to Harvard Law School. He later became the President of the US."
doc = nlp(text)

for ent in doc.ents:
    print(ent.text, ent.label_)
```
Output:
```text
Barack Obama PERSON
Hawaii GPE
Harvard Law School ORG
US GPE
```
n this code, we load the "en_core_web_sm" model in spaCy, and then apply it to a sample text consisting of three 
sentences. We then iterate over the sequence of Span objects in the resulting Doc object and print out the text and 
label_ attributes of each entity.

The label_ attribute contains the named entity label for each entity, which is a string that indicates the type of the 
entity, such as PERSON, ORG, or GPE (geopolitical entity). By analyzing the frequency and distribution of these labels 
in the text, we can gain insights into the types of entities that are mentioned in the text and their relationships to 
each other.

For example, we might use NER to:
- Extract mentions of people, organizations, and locations from a large corpus of text, which can be useful for building 
a knowledge graph or social network analysis.
- Identify mentions of specific products or brands in customer feedback or reviews, which can be useful for market 
research.
- Extract mentions of medical conditions or drug names from clinical notes or patient records, which can be useful for
healthcare analytics or research.

Overall, NER is a key task in NLP and is used in many applications, such as text classification, question answering, 
and information retrieval. By identifying and extracting named entities from text, we can gain insights into the 
underlying meaning and structure of the text, and use this information to build more accurate and effective NLP models.

## Dependency Parsing

Another thing we can do with spaCy is what is called getting a dependency structure for a sentence. Dependencies 
represent the grammatical relationships between tokens in a sentence. More specifically, a dependency is a directed 
relationship between a "head" token and a "child" token, where the head token is the word that governs the meaning of 
the child token.

Each token in a sentence is assigned a dependency label that describes its relationship to its head token. The most 
common dependency labels used in SpaCy are:

ROOT: The root of the sentence, usually the main verb.
nsubj: The subject of a verb.
dobj: The direct object of a verb.
iobj: The indirect object of a verb.
ccomp: A clause serving as the complement of a verb or adjective.
pobj: The object of a preposition.

There are many other dependency labels used in SpaCy, each with its own meaning and function.

Here's an example sentence with its dependency tree:

```python
text = "I ate pizza for dinner."
doc = nlp(text)
for token in doc:
    print(token.text, token.dep_)
```
Output:
```text
I nsubj
ate ROOT
pizza dobj
for prep
dinner pobj
. punct
```
Here, spaCy tells us that "ate" is the main verb of the sentence, that "I" is the subject of the sentence (marked as 
"nsub"), that "pizza" is the direct object of the verb (labeled as "dobj"), that "for" is a preposition and that 
"dinner" is the preposition's object.

Let's look at a more complex sentence:
```python
text = "The dog that chased the cat belongs to my neighbor."
doc = nlp(text)

# Print the dependencies for each token in the Doc
for token in doc:
    print(token.text, token.dep_)
```
Output:
```text
The det
dog nsubj
that nsubj
chased relcl
the det
cat dobj
belongs ROOT
to prep
my poss
neighbor pobj
. punct
```
In this sentence, the verb "belongs" is the root of the dependency tree. The subject of the verb is "dog", which is 
modified by a relative clause that begins with the relative pronoun "that". The relative clause has its own subject 
"that" and object "cat", and is connected to the main verb with the relcl dependency label. The "relcl" dependency label 
is used to indicate the relationship between the verb "chased" and the relative pronoun "that", which is its subject. 
The prepositional phrase "to my neighbor" modifies the verb and is connected to it with the pobj dependency label. 
The possessive pronoun "my"modifies the noun "neighbor" and is connected to it with the poss dependency label.

## Sentences and noun chunks
A spaCy doc is presented as a list of all tokens. But you can also get it as a list of sentences or a list of noun 
"chunks" (contiguous sequences of nouns and their modifiers):

```python
text = "Barack Obama was born in Hawaii and went to Harvard Law School. He later became the President of the US."
doc = nlp(text)
for sent in doc.sents:
    print(sent.text)
print()
for noun_chunk in doc.noun_chunks:
    print(noun_chunk)
```
Output:
```text
Barack Obama was born in Hawaii and went to Harvard Law School.
He later became the President of the US.
Barack Obama
Hawaii
Harvard Law School
He
the President
the US
```

## Word Embeddings
A final thing you can use spaCy to get is what is called a "word embedding". A word embedding is a vector representation 
of a word's meaning, calculating using machine learning algorithms that look at the context surrounding a word. For 
example, the vector representations for "dog" and "cat" will be similar to each other because they tend to appear in 
very similar contexts.

One way of describing this is that if you made a list of all the sentences that have "dog" in 
them, how many of those sentences could have had "cat" substituted for "dog" and still meant something very similar? A 
lot of them. But those same sentences could not have had "shoe" substituted and made sense.

A word embedding gives you a way to measure the meaning of words, and allows you to do things like compare the meaning 
of words and sentences for how similar they are. You can compute the similarity of two words' meanings by computing 
the correlation or cosine of their word embedding vectors.

Here is an example:
```python
import numpy as np

word_list = ["dog", "cat", "shoe", "sock"]

for word1 in word_list:
    vector1 = nlp.vocab[word].vector
    for word2 in word_list:
        vector2 = nlp.vocab[word].vector
        similarity = np.corrcoef(vector1, vector2)
        print(f"{word1} {word2} {similarity:0.3f}")
```
Output:
```text
dog dog 1.000
dog cat 0.822
dog shoe 0.335
dog sock 0.283
cat dog 0.822
cat cat 1.000
cat shoe 0.283
cat sock 0.261
shoe dog 0.335
shoe cat 0.283
shoe shoe 1.000
shoe sock 0.664
sock dog 0.283
sock cat 0.261
sock shoe 0.664
sock sock 1.000
```
Here we can see that a word's correlation with its own vector is 1, as it should be. Dog and cat have a high correlation
(0.822), and so do shoe and sock (though not as high, 0.664). The other correlations are all quite a bit lower. Though
they are still positive because these are all nouns.

Next: [12.3. Text Classification](../CH12/12.3.%20Text%20Classification.md)<br>
Previous: [12.1 Regular Expressions](../CH12/12.1.%20Regular%20Expressions.md)
