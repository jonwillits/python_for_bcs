# 12.3. Text Clustering and Classification

In this section we're going to talk a little more about some ways that you can apply machine learning to text.

## Document Clustering and Classification
One common technique in NLP is to take documents (defined broadly, we could mean sentences, paragraphs, books, 
emails, social media posts, etc.) and to cluster them based on their content. There are a number of different approaches
to this:

As a basic example, consider the following example. Imagine we had a bunch of social media posts, and we were interested 
in clustering their content. A simple way to do this would be the following:
- create a list of all the unique words across all the social media posts
- create an array for each post, tracking the word frequencies of each word in each post
- cluster those arrays

Here is some simple code that could do this, under the assumption the posts were all in a single file, one post per 
line:
```python
import spacy
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn import cluster

def get_posts(post_directory_path):
    # get a list of spacy docs for each social media post in a file
    doc_list = []
    nlp = spacy.load("en_core_web_md")
    with open(post_directory_path) as f:
        for line in f:
            line = line.strip('\n').strip()
            doc = nlp(line)
            doc_list.append(doc)
    return doc_list
            
def get_unique_tokens(doc_list):
    # get a dictionary with each unique token pointing to a unique id number
    unique_tokens = set()
    for doc in doc_list:
        for token in doc:
            unique_tokens.add(token.text)
    token_index_dict = {}
    for i, token in enumerate(unique_tokens):
        token_index_dict[token] = i
    return token_index_dict

def get_token_matrix(doc_list, token_index_dict):
    # create a matrix with a row for each document, and a column for each token, giving us the freq of each 
    # token in each document
    token_count_matrix = np.zeros([len(doc_list), len(token_index_dict)])
    for i, doc in enumerate(doc_list):
        for token in doc:
            token_index = token_index_dict[token]
            token_count_matrix[i, token_index] += 1
    return token_count_matrix
            
def cluster_documents(token_count_matrix):
    # run the clustering algorithm
    clustering_alg = cluster.MiniBatchKMeans(n_clusters=4, n_init="auto")
    clustering_alg.fit(token_count_matrix)
    y_pred = clustering_alg.predict(X)
    return y_pred

doc_list = get_posts("posts/")
token_index_dict = get_unique_tokens(doc_list)
token_count_matrix = get_token_matrix(doc_list, token_index_dict)
document_clusters = cluster_documents(token_count_matrix)
```
Now, this algorithm would probably not work super well, there would be a lot we'd want to do to improve it, such as: 
- clean the posts to remove text we didn't want to include
- doing normalization and data reduction to do our clustering on a smaller number of better structured dimensions

But the basics of doing things like detecting spam emails, or classifying posts on social media as harmful or offensive
start with algorithms like this that might find patterns in the words being used. If you had labeled data (say, for 
example a list of posts labeled as either offensive or unoffensive) you could use that dataset to build a classifier that 
would make guesses about other posts.

## Sentiment Analysis
One specific kind of document classification is called "Sentiment analysis". Sentiment analysis is an algorithm that can 
give you a score on a sequence of text telling you if that text is "positive" ("I love pizza!" or "negative" ("I hate 
broccoli!"). This is used in many practical settings to evaluate text. Sentiment analysis algorithms are trained 
in a way described above, just trying to predict each sequence of text categorically as positive polarity (+1) or negative
polarity (-1), or sometimes in a more standard regression format with a quantitative score ranging from -1 to +1.

The nlp package textblob is an easy way to get sentiment analysis scores. First install the textblob module. Then:
```python
from textblob import TextBlob

sentence_list = ["I really love pizza",
                 "I really hate broccoli",
                 "I am not sure how I feel about plums."
                 "Today is Monday."]

for sentence in sentence_list:
    blob = TextBlob(sentence)
    sentiment_score = blob.sentiment.polarity
    subjectivity_score = blob.sentiment.subjectivity
    print(sentence, sentiment_score, subjectivity_score)
```
Output:
```text
I really love pizza 0.5 0.6
I really hate broccoli -0.8 0.9
I am not sure how I feel about plums. -0.25 0.8888888888888888
```
Textblob can also give you a score for how "subjective" the sentence is, based on its training. How well do these 
work? Well you can play around with some in your lab...

Next: [12.4. Language Models](../CH12/12.4.%20Language%20Models.md)<br>
Previous: [12.2 NLP Modules](../CH12/12.2.%20NLP%20Modules.md)
